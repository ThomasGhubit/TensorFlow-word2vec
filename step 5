# Step 5: Begin training.
# 步骤五：执行训练
num_steps = 100001

with tf.Session(graph=graph) as session:
  # Open a writer to write summaries.
#   writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)

  # We must initialize all variables before we use them.
  # 初始化变量
  init.run()
  print('Initialized')

  average_loss = 0
  for step in xrange(num_steps):
    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,
                                                skip_window)
    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

    _, loss_val = session.run([optimizer, loss], feed_dict = feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
      if step > 0:
        average_loss /= 2000
      # The average loss is an estimate of the loss over the last 2000 batches.
      # 2000个batch平均损失
      print('Average loss at step ', step, ': ', average_loss)
      average_loss = 0

    # Note that this is expensive (~20% slowdown if computed every 500 steps)
    # 每1万步，进行一次验证
    if step % 10000 == 0:
    # sim是验证词与所有词之间的相似度
      sim = similarity.eval()
      close_word_collect = [[0 for col in range(5)] for row in range(5)]
      similarity_collect = [[0 for col in range(5)] for row in range(5)]
      for i in xrange(5): # 一共有valid_size个验证词
        valid_word = reverse_dictionary[word_num[i]] #reverse_dictionary[valid_examples[i]]
        top_k = 8  # number of nearest neighbors 输出最相邻的8个词语
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = 'Nearest to %s:' % valid_word # word_list1[i]
        close_word_collect[i] = []
        similarity_collect[i] = []
        for k in xrange(top_k):
          close_word = reverse_dictionary[nearest[k]]
          close_word_collect[i].append(close_word)
          similarity_collect[i].append(sim[i, nearest[k]])
          log_str = '%s %s: %s,' % (log_str, close_word, sim[i, nearest[k]])
        print(log_str)
        
#     if step % 10000 == 0:
#       sim_ = similarity.eval()
#       close_word_collect = [[0 for col in range(5)] for row in range(5)]
#       similarity_collect = [[0 for col in range(5)] for row in range(5)]
#       for i in xrange(5):
#         check_word = reverse_dictionary[word_num[i]]
#         top_k_close_word = 8
#         nearest_ = (-sim_[i, :]).argsort()[1:top_k_close_word + 1]
    
#         log_string = 'Nearest to %s:' % check_word
#         close_word_collect[i] = []
#         similarity_collect[i] = []
#         for k in xrange(top_k_close_word):
#           close_word_ = reverse_dictionary[nearest_[k]]
#           close_word_collect[i].append(close_word_)
#           similarity_collect[i].append(sim_[i, nearest_[k]])
#           log_string = '%s %s: %s,' % (log_string, close_word_, sim_[i, nearest_[k]])
#         print(log_string)

  final_embeddings = normalized_embeddings.eval()
